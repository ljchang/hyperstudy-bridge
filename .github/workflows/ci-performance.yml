name: Performance Testing & Benchmarks

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  unit-tests:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'

    - name: Install dependencies
      run: npm ci

    - name: Run unit tests
      run: npm run test:coverage

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}

  rust-benchmarks:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
        components: rustfmt, clippy

    - name: Cache cargo registry
      uses: actions/cache@v3
      with:
        path: ~/.cargo/registry
        key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}

    - name: Cache cargo index
      uses: actions/cache@v3
      with:
        path: ~/.cargo/git
        key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}

    - name: Cache cargo build
      uses: actions/cache@v3
      with:
        path: src-tauri/target
        key: ${{ runner.os }}-cargo-build-target-${{ hashFiles('**/Cargo.lock') }}

    - name: Run performance benchmarks
      run: |
        cd src-tauri
        cargo bench --bench performance -- --output-format json | tee ../benchmark-results.json

    - name: Run stress tests
      run: |
        cd src-tauri
        cargo bench --bench stress_test -- --output-format json | tee ../stress-test-results.json

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: rust-benchmarks
        path: |
          benchmark-results.json
          stress-test-results.json

    - name: Performance regression check
      run: |
        # Compare with baseline performance (implement custom script)
        echo "Checking performance regression..."
        # This would typically compare against stored baseline metrics

  e2e-tests:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'

    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true

    - name: Install dependencies
      run: |
        npm ci
        npx playwright install --with-deps

    - name: Build Tauri app
      run: |
        cd src-tauri
        cargo build --release

    - name: Run E2E tests
      run: npm run test:e2e
      env:
        CI: true

    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: playwright-report
        path: playwright-report/
        retention-days: 30

  load-tests:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'

    - name: Setup Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true

    - name: Install dependencies
      run: npm ci

    - name: Start bridge server (background)
      run: |
        cd src-tauri
        cargo run &
        sleep 10  # Wait for server to start

    - name: Run light load test
      run: npm run test:load:light

    - name: Run heavy load test (on main branch only)
      if: github.ref == 'refs/heads/main'
      run: npm run test:load:heavy

    - name: Upload load test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: load-test-results
        path: load-test-results.json

  performance-report:
    runs-on: ubuntu-latest
    needs: [unit-tests, rust-benchmarks, e2e-tests, load-tests]
    if: always()

    steps:
    - uses: actions/checkout@v4

    - name: Download all artifacts
      uses: actions/download-artifact@v3

    - name: Generate performance report
      run: |
        # Create a comprehensive performance report
        cat > performance-summary.md << 'EOF'
        # Performance Test Report

        **Build**: ${{ github.sha }}
        **Date**: $(date)
        **Triggered by**: ${{ github.event_name }}

        ## Test Results Summary

        ### Unit Tests
        - Status: ${{ needs.unit-tests.result }}

        ### Rust Benchmarks
        - Status: ${{ needs.rust-benchmarks.result }}

        ### E2E Tests
        - Status: ${{ needs.e2e-tests.result }}

        ### Load Tests
        - Status: ${{ needs.load-tests.result }}

        ## Performance Metrics

        <!-- Load test results will be parsed and inserted here -->

        ## Recommendations

        <!-- Performance improvement recommendations based on results -->

        EOF

        # Parse load test results if available
        if [ -f "load-test-results/load-test-results.json" ]; then
          echo "## Load Test Details" >> performance-summary.md
          echo "" >> performance-summary.md
          echo '```json' >> performance-summary.md
          cat load-test-results/load-test-results.json >> performance-summary.md
          echo '```' >> performance-summary.md
        fi

    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: performance-summary.md

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          if (fs.existsSync('performance-summary.md')) {
            const report = fs.readFileSync('performance-summary.md', 'utf8');

            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
          }

  performance-dashboard-deploy:
    runs-on: ubuntu-latest
    needs: [performance-report]
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4

    - name: Deploy performance dashboard to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./src/test
        destination_dir: performance-dashboard
        enable_jekyll: false

    - name: Update dashboard with latest results
      run: |
        # Update performance dashboard with latest benchmark results
        echo "Dashboard deployed to GitHub Pages"